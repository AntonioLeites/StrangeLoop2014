Strangeloop 2014.

"Consistency without concensus in production systems"

Presenter:  Peter Bourgon

Live captioning by Norma Miller, whitecoatcaptioning.com.


>>...   and this happens like all the time.  If you interact with any distributed system, you're totally used to seeing this, right?  It's not in any way abnormal, there's a myriad of things that can go wrong and they're all things we have to account for when we're interacting with these systems and especially building them.  So I'm not the first guy to recognize this.  Ever since there were networks, there have been networks -- network programmers and out of that, morass we've developed so-called idioms.  Idioms to deal with the complexity of distributive programming and over the years they've evolved.  So I'd like to lay out a lineage, I'd like to start in the 80s, RPC is a remote procedure call and the idea is you can monitor network communication as a function call, somehow, but I want to argue there's a big difference between moving a program counter in a CPU and serializing a request in a network, shipping it off and sequencing it.  While this worked for a while, kind of it, it wasn't a really powerful idiom and it didn't let us solve a lot of the problems we needed to solve.  So I'm going to argue next up in the 90s is we had these abstractions like CORBA, has anybody ever used CORBA before?  I see some hands and I'm so sorry.  CORBA was this terrible thing.  Well, OK, it gave us a lot of promises, right?  It produced these higher level abstractions, it had this thing called the distributed object paradigm.  I think it tried to say that if you have a reference to some data, some object, it shouldn't matter if it was on your local machine or across the Atlantic, you could interact with it in the same way, that's what it promised at least.  But I want to argue that that abstraction went too far too fast.  CORBA didn't really deal with the errors that naturally arise in the network as it should have.  I think that it treated the network as more reliable than it actually was.  And as a result, it didn't provide a good robust, error-handling mech mechanism.  If you're checking your bank balance with bank America these days u' probably going through some CORBA systems.  So that's all we had until the year 2000 came.  In the year 2000, we had the CAP.  I'm not really qualified to talk in great detail about the cap theorem because I'm not that smart, but I will say that it represented a fundamental way in the way we shifted the way we were thinking.  It made us acknowledge and confront the reality of partitions reifying this thing called partition tolerance.  You've probably seen this diagram, right?  The three they were meant to be circles, they're kind of ovals there, these thee concepts, partition tolerance, consistency and availability.  And what CAP told us is that we couldn't choose all three, right, so this little bit was out.  So we wanted to maximize the things that we do cover and what CAP.  Leaves us with two optimization, we can choose this little bit here and have a so-called CP systems, or have this system AP.  CP systems:  They're typically built using something called a consensus protocol, which guarantees strong consist tense why I up to a certain level of  failure.  So as some examples, some systems like chubby or Kooser use the Paxos concensus protocol.  
>> The latest and greatest hipster consensus protocol is raft.  There's a talk a little bit later today I think, you should go see that definitely.  There's another protocol that called view stamped replication but I'm not sure of anything that actually uses it.  Does anybody know actually?  If there is one, I'd be really curious to know.  It's a fun read if you're into that.  So these protocols give strong guarantees and they're kind of provably correct, right?  But they're difficult to explain, difficult to debug and maintain.  Also slow in the sense that a single transaction has high latency and a set of transactions has low overall through-put.  So they're unsuitable for some transactions.  For example, you are wouldn't probably store a tweet stream using a CP system.  For that we have to go to the to the flip side and look at AP systems.  So most distributed data systems I'm going to argue are basically AP.  And these don't actually sacrifice consistency.  They just use a different form of it, so-called less than strong consistency.  Of those types there's one that's called a ventral consistency, which you probably all have heard of or are familiar with.  And that states that you want to accept that some nodes can be temporarily stale, but you want to be sure that the system converges toward a consistent state.  We started seeing systems like this built, but they were built with different systems of a ventral consistency.  As a result, there was this large gap between the customer expectation, the users of these systems, what they expected them to do, and what the systems could actually do, the actual capability.  And as a result, we saw some public failures pretty embarrassing public failures in some of these systems as we kind of worked out what it actually meant to be eventually consistent.  So I'm going to argue that this was all due to a lack of a good theoretical foundation.  And by that I mean that the systems that were kind of homegrown, they didn't really account for failure modes as a robust a way as they should have.  So what do I mean by like failure modes?  What does it mean for messages to fail?  There's a lot of ways, right?  In our networks, messages can be delayed.  They can be dropped altogether.  They can be delivered out of order and they can be duplicated.
>> And these are invariants in our networks, right?  We have to deal with them because they're always going to happen.  One way to deal with them is to abstract them away and that's what CP systems do.  They put a layer on top that compensates for all the things that can go wrong and presents like a stable API to the user.  But maybe a simpler and a better way is to allow these things to happen and that's generally what API systems do.  The question is how do we do that without corrupting systems data?  How do we do that without having these really public, embarrassing failures?  And we didn't really have a good answer to that question for a long time, but in the late 2000s -- I think around 2004, 2006, something like this -- some things started to become public.  And that's what I really wanted to talk about.  The first thing that started to appear is this thing called the CALM principle.  Has anybody heard of this?  A couple of people, OK.  For the rest of you consistency as logical monotonicity.  Which is a mouthful.  I guess the best way to describe this is that systems should only grow in one direction.  So you can think of a counter that only gets bigger.  There's a language called bloom lang which is became on this principle.  But this is a pretty abstract.  I want to present another example that's a bit more concrete and that is this concept of ACID, so in database theory ACID has a particular definition and I always forget what it is:  So that's existed for a long time.  In 2009, I think this really smart guy whose name escapes me at the moment, said aha, I can reuse this acronym for distributed systems and for that we'll say that ACID means associative commutative idempotent, and he said distributed sure, whatever.
>> And it means all the operations in your distributed system should abide these properties and if they do, then you have something special and what that special thing is, I think, is a CRDT.  This is even a further concretization of the principle.  A CRDT is a conflict-free replicated data type and it's a distributed data type, which is provably eventually consistent, key, provably.  Without strong consensus.  OK, that was a lot of words, what does it all mean?  Let's go with an example.  As an example, a motivating example, let's consider the case of an increment-only counter.  So a counter is something that can go up and down, an increment-only counter can go down or up.  So to use a more academic term of art it's a register that supports two operations.  Increment, by one, or read the current value.
>> OK, so let's think about the naive way to implement this.  The operation you would probably use, well, to read is easy, right?  You just read the value.  But to write the operation you would probably use is addition.  OK, so let's look after the addition.  Is addition associative?  Yes.  That's true.  Clear.  Is it commutative?  Yes, that's also true.  But unfortunately it's not idempotent, so it's not ACID, it's not CALM compliant, and addition by itself cannot make a CRDT, so let's shift our thinking a little bit and rather than thinking of pure integers, let's think of another data type, specifically the set and when you're in the domain of sets, the addition operator is union.  So let's do this again.  If you have a set containing one and two, and you union it with a set containing 3, well, that statement is true, so it's associative.  That statement is also true, so it's commutative, and luckily that statement is true, so it's all three, set addition, which is union, passes are crucible, so the operation is like plus 1, right, so there's no unique thing that you can insert into a set with just that operation.  Right?  Plus 1, plus 1, plus 1, the ones are all the same so to speak.  So what we have to do is think a bit.  We have to maybe consider the actual problem domain, not some abstract thing like a counter which can be used for anything.  Let's consider a specific example, a counter to count a specific thing.  So I work at SoundCloud and one thing we have is tracks.  So you can upload a track to the platform and then maybe what you're interested in doing is counting the plays on that track, unique plays, so you can see how many people have heard your track.  Indeed we do this, it's an important part of the platform.  So if if you had an incremental counter for that track, what would it look like, what can you leverage to make it abide by set semantics?  And if you think about it, the person who played the track, they might have a user ID and we can use that as the unique key that we insert into our set.S so, OK, let's try that out.  Let's see what it looks like.  Let's say we had a distributed system that increments this increment-only counter and let's say they had three nodes.  So here they are.  Empty at the moment.  And let's consider what it looks like when it looks like when somebody starts to play that track and we want to implement that counter.  So this is a specific distributer called a G set, I think.  We take their user ID and we say this use user has played that track.  We insert that value into that set and now that set has a cardinality of 1 and the overall system is inconsistent, so at this moment that would read from the first to the third node, we would see a value of 0, and if we did the next one we would see the value of 1.  And that's fine.  So it's going to distribute its value to the other nodes.  It's going to look like this.  Same thing here and now we are in a consistent state, cardinality of 1, value of 1 across all nodes.  OK, easy enough.  Let's say another user comes-by-and plays the track.  Same thing happens here, let's say the first distribution message fails.  That one succeeds, and now we're in this state, right?  So we're not consistent right now.  And that's kind of bad, but what we can can do here is leverage a nice property of sets.  And we can say what happens during a read?  So let's say that we read -- someone comes in and does a read, and we actually do all three nodes at once.  This doesn't happen every time but it can happen occasionally, like with 10% probability, let's say and we read all the values out and this is what we get.  One node yields both elements, the third node yields both elements and the second node yields only 1 so because union has this nice property of being idempotent and so we can return to the client and say the number of plays on this track is 2.  But we can also do another set operation which is nice and I think it's called the symmetric difference and we can say the symmetric difference of those three sets is those elements which are not in perfect agreement and in our case it's 456 and what that says is 456 is missing somewhere and so what we can do is say aha, there's inconsistency, so I write 456 into the system.  And remember, union is idempotent, so this has no effect.  So this is very primitive Ford of read repair, I suppose, and it's the way that this particular set, one way this particular set can achieve eventual consistency.  So that's cool.  We've bypassed a lot of the inherent problems in distributed systems, or rather, the inherent ways that networks fail by bending our problem a little bit, and so I'd like to maybe just take a brief interlude and talk about this.  So CRDTs are this solution to a problem, and the solution has these really nice properties, right?  It lets us abstract away a lot of the problems of networks, but that's in this case, it required bending our problem definition to fit the solution.  So do you remember like -- I don't know how many years ago it was where someone came up with a term like dev ops and it was a revolution, right and so here's how I interpreted it.  In the bad old days, in the 90s, or whatever, the way software shops worked is you would work on your laptop and you would build your artifact or your war file or whatever and you'd say OK I'm done and you'd throw it over to the wall to your system and they would spin it up and it would crash and 2 it wouldn't play nicely with everything else and who is here's the new version, it's totally different behavior and as a result the software we built in those days wasn't really that awesome, it could have been a lot better, right and so we acknowledged this at some point, and we acknowledged when I think what I'm going to argue is an invariant, which was that developers are the ones who are best qualified to run their code in production, right, because they wrote it, they know what its failure modes are, they know how it breaks down, they know how to upgrade it, they know how to manipulate it, that sort of thing.  So when we recognized that variant, we had a revolution of type.  And defs are the ops.  Defs are the ones who should be fixing it and getting paged when it breaks, defs are the ones who are scaling it up when the load requirements grow and scale it down when they don't and as a result I want to argue that our software is a lot better.  I mean we at SoundCloud work this way.  I hope you all work this way.  If you don't, it's bad, you should feel bad.  But this is good stuff, right?  This is overall a benefit to the software industry.  So I want to make a similar case for distributed programming.  I want to say in the bad old days we wrote distributed programs very optimistically, we assumed the network was a lot more reliable than it was and even though we knew it wasn't, we swept it under the rug.  Eventually we discovered these variants and the CAP.  We mapped it out a bit, and the revolution, I want to argue is that we can build these eventually consistent systems, using tools like provably correct tools like CRDTs and as a result our distributed systems can be a lot more reliable and we can just produce better software in general and sometimes that requires bending the problem, rather than trying to hammer it into a square-shaped hole.
>> OK.  That's my little sermon, so now to the meat.  CRDTs in production.  I said a couple times I work at SoundCloud.  Has anyone heard of SoundCloud, or am I just -- OK, great, we are a website, I guess, we put your sounds in the cloud.  If you go to SoundCloud and you like create an account, and then you like follow some music and audio creators, then you got to a page that looks like this and this is called our stream and the idea here is it's basically similar to your Facebook feed or your Twitter stream or something like that, you follow people and whenever they do stuff, you see it, just strict chronological order.  Everything that appears in here has basically the same form.  It's basically an event.  And events all have basically the same properties.  So an event is a time stamp of when it happens.  The user who did the thing.  The verb of the thing that was done, like for example a track could be uploaded or a play list could be reposted or something like this.  And then there's the actual thing that the operation was done on.  And this captures every possible thing that can appear in the stream, so for an example, at 2014 blah blah blah, Snoop Dogg might have reposted this economist podcast as unlikely as that might be.  OK, so that's an event, right, and what's nice about an event is that it's very small in theory and it is unique, Snoop Dogg can only repost that economist podcast once, I mean he only does it or he doesn't, right?  So that's a nice property that will come handy later.
>>
>> The first way I think is so-called fan out on write.  So on this model, every user gets a sort of inbox and whenever a creator does something, that's action is copied to all of his or her followers' inboxes.  So for example, when Snoop Dogg reposts the economist podcast, everyone who follows Snoop Dogg is going to get a copy of that event in their inbox, so there it is, and it's going to be copied here and there and there, I mean as far as I'm aware, this is how Twitter does it, this is how Facebook does it and it seems intuitive, right?  But there are some problems here.  The biggest problem is just the massive amount of data that is generated.  I mean you can imagine we don't obviously cap the number of followers you can have, so if someone with 2.5 million followers does one thing, that's 2.5 distinct writes somewhere.  There's also other problems.  Problems of dynamism, so if you unfollow somebody, obviously you want all of their stuff to be out of your inbox, but that's a hard problem, because we have to like walk this potentially very long list of things and cut things out or do some sort of post filtering or something like that.
>> There's a couple other product level things that make this kind of not a desirable situation, so the alternative is so-called fan in on read, and this just inverts the relationship, so instead of everybody having an inbox that you write into, and this all the creators have an outbox, so here we have two creators with sets of things hand whenever we load our stream, what we want to do is check all the people who we follow, go to their outbox and kind of materialize a view dynamically so that kind of looks like that, that, something like this.  Now, this is actually much nicer, because it means every action that a user performs on the platform, it only gets one right, it's only written to their outbox.  Since the streams are generated dynamically we can do fun things, we can do for example, insert recommended content, we can reorder things if we think it's a good idea, but it does come with a cost, right, and that that read, merge and display has to be done within the typical-response timeline, which means something on the order of single-digit milliseconds as a high upper bound, so that's tricky.  But if we could do it, we get a lot of product benefits, so that was the challenge about a year ago, I guess at SoundCloud.  The existing fan out on write-in infrastructure was running out of its usable life and we had to either extend it or maybe try a new paradigm.  So what we wanted to try to do is have a fan-in on read architecture that was maybe CRBT based and see how far we could get.  So as I alluded to, we had unique events and so maybe we could use a set, a CRDT set, it turns out there are several types of CRDT sets, the G set is great but you can't delete.  So that's not awesome.  There is something called a 2P set which is something you can delete from but only once.  There's something called an OR set which you can delete from indefinitely, but it comes with a storage overload which is not great.  So what we did was create a new set, it's not totally new but we called a Roshi set for reasons I will get into later if you want to talk to me after the talk.  It has a single logical set is represented by two physical sets under the covers.  So for example, here's one physical set, the so-called add set.  It may have these elements.  Then there's also a remove set which represents the elements that are not in the logical set and you do a semantic merge of the tough and it kind of looks like this.  Technically this is a modified lacerater within the element set which a garbage collection.  We'll get to that in a bit.  So what do all these numbers and letters mean?  You could imagine -- let's consider Snoop Dogg again.  S would be his outbox key show that would be for example his user ID.  The letters A, B, C or D, would be the unique combination of things that represent the event and the 1, 2, 3 is the time stamp so the time at which Snoop Dogg did his thing.  So if this is our model, then it turns out reading is easy from a single logical node to read events, you just look at the add set and you return it directly to the user.  That's fine.  What's interesting is writing.  So let's look at that.  Let's look at doing an insert.  The algorithm is this:  If either the add set or the remove set already contains the element, and the existing score is greater than or equal to the incoming score, then that's a no-open-ended and we exit.  We return success and we're fine.  We don't do anything.  Otherwise, if this condition wouldn't true.  We insert that element from the add set and we delete any matching element from the remove set.  We just swap these two sets at the bottom like that.  That's a bit obscure, so let's run through a quick example.
>> So let's say this is our state.  We have an add set with A and B and a remove set with B.  So here comes an operation.  Insert D with a score or rather a time stamp of 4.  What happens?  D does not exist anywhere so it's free to be inserted so it goes into the add set and we're done.  OK but of course we operate with broken networks, so that message was duplicated and we have the same thing.  Insert D4.  What happens is we scan both of the existing sets, we see that D already exists with a score of 4, so this is a no-op and an exit.  Now we have a delayed message.  A delete of D with an older time stamp of 3.  Again we scan both existing sets.  We see that it's been beaten.  A score of 4 already exists.  So this is a no-op and a delete and exit.  Now we have a message that comes in with a updated time stamp, 5 beats 4 so it goes into our delete set.  Similarly for whatever reason, if we had a delete with a score of 6, we would update of D in the create set and achieve a final state.  This is how I think of CALM and ACID, is you take any set of unique operations and you throw it in a big bag and you if your distributed system is ACID and calm compliant, then the end result of that, no matter how many times you do it from the same initial state, you'll always get to the same final state and think from about that from an operational point of view.  I don't know if you're like on call for stuff, right, but imagine if your system were so robust that you didn't have to worry about like lining up partitions when you needed to replay bad data or replay missed data or something like this, it's really powerful stuff and it makes my life a lot easier as the operator of this system.
>> So that example looks really cool in keynote slides but how does it work in real life?  Do we need to invent new software to build this system or can we use existing software?  It turns out there's an existing piece of software that works really well and it's called Reddis.  It gives us exactly this like score-based add and remove operations, Reddis has atomic scripting so and it works great, right?  Of course Reddis is an in-memory data structure server and SoundCloud has more data that did fit on a single machine, incredible as that might seem, so one Reddis isn't enough and we have to have multiple Reddis, which isn't such a big deal.  But we decided to do it in a slightly different way.  We have this natural nice shard key which is our set ID, so it's like Snoop Dogg's user ID, for example, so we use that to shard across multiple Reddis instances.  They are noncommunicating, they don't speak to each other.  To do communication, we have a layer on top of them, which we've called the pool.  And all that does is say given a single key which instance owns it, which instance do you interact with?  OK?  On top of pool, we implement another layer called a cluster, which provides the CRDT semantic API, so that's insert, delete, and select, read, and that's cool.  So here we have like effectively one node, right?  Snoop Dogg's outbox only exists once in all of these Reddis instances, and that's great, that works, but it's not, if any of the Reddis instances die, we have data loss, right?  So we want to make this highly available and we want to replicate it like you do, so what we did is exactly that.  Replicate that stack.  But these independent logical nodes don't communicate with each other.  Unlike Cassandra, for example, they might gossip between each other, here we control everything with kind of a master, a command and control layer which we called the farm.  So queries go in, hit the farm, the farm does something to interact with the individual clusters and then return responses.  So at this level, writing is actually very easy.  Every write follows the same pattern, you want every write to hit the same pattern eventually, so you copy writes to all clusters.  That's our version of write form.  Generally you want 51%.  OK, but reading is interesting, because we have several options, right?  So here we are, a read request hits the farm.  What's the most naive thing you can do?  You can send it to just one cluster.  That's great, it's fast, it's obvious but it doesn't give you much durability.  If that cluster happens to be out of sync, not consistent, then you're going to reflect back to the client, not great.  So the other extreme is to send it to all the clusters, do.  So for example, if this is what gets returned, then you can do the union, return that to the client, it's provably correct, you do the you can see every instance:  So these are the two extremes, right, the really naive one and the really complex one, but this carries a cost, right?  The cost is increased network back and forth, and CPU burn and that sort of thing.  So what we do is this hybrid mode where like 90% of the time, let's say, we just return the first responder, kind of assuming that it's going to be in a consistent state, but we launch kind of a listener in the background and we wait and collect all the responses, and we won't return anything to the client from that guy, but we will compare, and if we see any inconsistency, then we send read repairs to the correct notes.  If you know anything about Cassandra, this is basically how Cassandra works.  In fact the criticism of the system is we've implemented a mini Cassandra.  And speaking of that, this whole thing is like public and open source, so if this was at all interesting to you, take a look.  I think I've described pretty much all of the moving pieces so this should be very familiar to you if you've listened to this talk.  OK, so I mean that's our system in production.  So just a few concluding remarks.  About CRDTs.  That is this:  No. 1 if you think of consistency without consensus, I want you to think of CRDTs, they're the current best practice, their' the current cream of the crop and they're as of today how you should be implementing eventual you Co. consistency, I'd like to argue.  The other thing that I want to argue that we should embrace our invariants and not try to distract them away.  Especially hard invariants like faulty networks or things of this nature.  I think that if you do that, and you build your systems with the invariants in mind, we build much better software as a community, and on that same note, maybe it makes more sense to bend your problem description rather than trying to hold it fixed and bend a solution domain to solve it.  If you embrace your invariants and you do that, I think it makes a lot more sense.  OK, that's it.  Thanks very much.
AUDIENCE MEMBER:  [inaudible]
>> So the question was, when we do, when do we compute tombstones which is a term of art for the entry and delete set, essentially, and yeah you're correct it's as soon as the delete hits, we do this atomic element.  And we either remove it to the delete set or -- so many, I'm sorry.
AUDIENCE MEMBER:  [inaudible]
>> So the question was how do we deal with clock drift and the answer is we don't, haha.  No, that's the cheat in the system for sure.  We presume that the guy sending us the events, we take their time stamp as like the pure honest truth and we kind of defer that question to them.
>> So, yeah, out of scope, sorry.  Maybe one more?  One more question?  I guess that's what everybody was asking.  OK, yeah.
>>
AUDIENCE MEMBER:  [inaudible]
>> Yeah, so the question was, since we only reconcile consistency problems during reads, if somebody doesn't have any followers, they could eventually become very inconsistent if there are no read.  That's true.  So we have another component which I didn't describe, haha, which is called a walker process which basically starts at one end of the key space and reads every key sequentially and then loops back and does it again.  And this is run independent of the rest of the system and at a variable rate, so basically for us, over the course of about 8 hours, we scan the entire key space, and ensure consistency in that way.  Yeah, cool.  OK, I'm now over time, so thanks again.
[break]

