Strangeloop 2014

"Transducers"

Presenter:  Rich Hickey

Live captioning by Norma Miller, whitecoatcaptioning.com


 ALEX:  It's a great pleasure to introduce Rich.  He's became a friend of mine a few years ago, and I have the great pleasure to work with him every day and it's a true joy.  

RICH:  And I'm Rich Hickey, the creator of Clojure and Datomic.  How about taking 10 seconds to thank Alex for another amazing conference. All right, transducers.
AUDIENCE MEMBER:  Yeah!
     Of course, everything is just some some combination of the same ingredients and the shell is on the inside, the cheese is in, it's on top, whatever.  I'm not claim any novelty here, this is just another rearrangement of the same old stuff as usual, but you know, sometimes the cheese on top you know, tastes better than when it's inside.  All right.  So what are transducers?  The basic idea is to go and look again at map and filter, and see if there's some idea inside of them that could be made more reusable than map and filter, because we see map and filter being implemented over and over again in different contexts, right?  We have map and filter on collections, map and filter on streams, on observables.  And there's just no sharing here, there's no ability to create reasonable things.  So we want to take the essence out and see if we can reuse them and the way that we're going to do that is by recasting them as process transformances and I'll talk a lot more about that but that's essentially the idea.  
     Recasting the core.logic of what were sequence processing functions as process transformations and then providing context in which we could host toes transformations.  So when I talk about processes, what am I saying not all processes can be modeled this way but there are a ton of processes that can and the critical words here are if you can model your processes as a succession of steps, right, and if you can talk about a step or think about a step as ingesting an input, as taking in or absorbing some input, a single input, so something going on, there's an input, we're going to absorb that input into the something going on and proceed.  That's the kind of process that we can use transducers on.  And when you think about it that I that way, building a collection is unfortunate judge one instance of a process with that shape, all right, building a collection is you have the collection so far, you have the new input and you incorporate the new input into the collection and you keep going.
>> Right?  But that's a specialization of the idea.  The general idea is the idea of a seeded left reduce, of you know, taking something that you're building up in a new thing and continually building up but we want to get away from the idea that the reduction is about creating pa particular thing and focus more on it being a process.  Some processes build particular things, other processes are infinite, they just run indefinitely.  So we made up words, actually we didn't make up a word, again this is actually a word.  But why this word?  Well, we think it's related to reduce and reduce is already a programming word and it's also already a regular word and the regular word means to lead back, right?  To bring something back and we've sort of -- the word has come to mean over time to bring something down or to make something smaller, but it doesn't necessarily mean that.  It just means to lead it back to some, you know, mother ship and this process that we're trying to accomplish.  The word ingest means to carry into, so it's the same idea.  But that's about one bite, and transduce means to lead across.  And the idea basically is, as we're taking inputs into this reduction, we're going to lead them through a series of transformations, we're going to carry them across a set of functions so we're going to be talking about manipulating input during a reduction.
>> So this is not a programming thing.  This is a thing that we do all the time in the real world.  We don't call them transducers, we call them instructions.  Right?  And so we will talk about this scenario through the course of this talk, which is put the baggage on the plane.  OK, that's the overall thing that we're doing, but I have this transformation I want you to do to the baggage, all right?  I want you to while you're doing it, while you're putting the baggage on the plane, break apart the pallet, so we're going to have pallets, big wooden things with a pile of luggage on it that's sort of shrink-wrapped.  We want to break them apart so now we have individual pieces of luggage.  We want to smell each bag and see if it smells like food.  If it smells like food, we don't want to put it on the plane.  And we want to take the bags and see if they're heavy and we want to label them.  That's what you have to do.  So we're talking to the luggage handlers, we say that's what you have to do and they all say great, I can do that.
>> Well, the really important thing about the way that was just said and the way you talk to luggage handlers and your kids and anybody else you need to give instructions to, is that the conveyance and the source, and the synchs of that process are irrelevant, right?  Do the luggage handlers get the bags on a conveyer belt or on a trolley?  We didn't say.  We don't care.  In fact, we really don't want to care.  We don't want to say the luggage guys today, there's going to be luggage on a trolley, do this to it and then put it on another trolley and tomorrow when we switch conveyor belts say, we didn't know what to do.  You know, it came on a conveyor belt and like, you didn't -- you know, I have rules for trolleys.  So the rules don't care.  The instructions don't care.  Right?  This is the real world, right?  Then we have programming.  What do we do in programming?  We have collection function composition.  We're so cool, you know, we have lists, we have functions from list to list, right?  So we can compose our functions, we're going to say, well, labeling the heavy bags is like mapping, right?  Every bag comes through and it gets a label or doesn't.  But for every bag that comes through, there's a bag that comes out and maybe it has a label or doesn't.  And taking out the nonfood bags or keeping the nonfood bags is a filter.  So we may or may not have an input depending on this predicate and unbundling the pallets is it like map cat, there's some function that gives you a pallet, gives you a whole bunch of pieces of luggage.  So we already know how to do this.  We're done, we're finished, programming can model the real world.  Except there's a big difference between this and what I just described happens in the real world, right?  Because map is the function from whatever, collections, to collection or sequence to sequence or you know, pick your programming language but it's basically a function of aggregate to aggregate and so is filter and so is map cat and the rules that we have only work on those things, they're not independent of those things and when we have something new like a channel or a stream or observable, none of the rules that we have apply to that.  And in addition, we have all this in between stuff.  It's as if we said to the luggage guy, take everything off the trolley, right?  And unbundle the pallet and put it on another trolley, right?  And then take it off that trolley and see if it smells like food and if it doesn't, put it on another trolley.  And take it off that trolley and if it's heavy, put a label on it and put it on another trolley.  This is what we do in programming.  We do it all the time and we wait for a sufficiently smart supervisor to come and say, like, what are you guys doing?  Right?
[laughter]
>> So we don't -- we don't want to do this anymore.  Right?
>> We don't have any reuse, right?  Every time we do this, we end up writing a new thing, you write a new kind of stream, you have a new set of instructions, right?  You invent RX, boom there's a hundred functions.  We were starting to do this in Clojure, right, we had channels hand we're starting to write map and filter again so it's time to say time out, can we do this.  Because there are two things that can happen.  One is all the things we're doing are specific and the other is there's a potential inefficiency here, right?  Maybe there's sufficiently smart compilers and maybe for some context they can make the intermediate stuff go away, maybe they can't.  The problem is our initial statement really doesn't like what we normally do.  It's not general, it's specific.  We're relying on something else to fix it, right?  And we also have this problem, right, where we're going to go from one kind of conveyance to another and now all of a sudden you know, map is from x to x, and you know, how do we fix this?  And I know what everybody is thinking, of course.
[laughter]
>> Yeah.  So I mean -- so that may fix some of this, but in general, T. doesn't solve the problem, and the problem is mostly about the fact that we're talking about the entire job, right?  Those instructions, they were about the step.  They weren't about the entire job.  The entire job was around it.  While you're doing this thing, here he has what you're going to do to the inputs.  Here's how you're going to transform them while you're doing the bigger thing which could change.  So we want to just take a different approach.  If we have something that's about the steps, we can build things that are about the whole jobs, but not vice versa.
>> OK.  So just going to be some usages here, and then I'll explain the details in a little bit, because usually I do it the opposite way and people are like, oh, my brain hurt for so long and then like 40 minutes in you showed me the thing that made it all valuable.  So here's the value proposition.  We make transducers like this, we say I want to make a set of instruction, I'm going to call it process bags.  I'm going to compose the idea of map catting using unbundled pallet as the function.  So I want to unbundle the pallets, then I want to filter out the nonfood or keep the nonfood, filter out the food and I want to label the heavy bags.  And in this case we're going to pose compose those functions with comp, which is order function composition thing.  So map catting filtering and mapping return transducers and process bags which is the composition of those things, is itself a transducer.  Recall map catting, recall filtering, call mapping, get three transducers, compose them and make another transducer right?  Each transducer takes a process step, right or its reducing function and transforms it, changes it a little bit.  So before you do that step, do this.  I'll explain why that seems backwards in a little bit.
>> Having made those instructions, we could go into completely different contexts and reuse them, right, amongst the several contexts that we're supporting in Clojure in the first version is supporting transducers and into and into is Clojure's function that takes a collection another collection and pours one into the other.  We just had the stand alone thing called into, but it's the same idea, your source and destination could be different so we want to pour the pallets into the airplane but we want to take them through this processed bags transformation first, so this is collection-building, into is already a function in Clojure, we just added an additional area that takes transducer, then we have sequence, sequence takes some sort of stuff and makes a lazy sequence out of it.  Sequence now takes a transducer and will perform that transformation and all the stuff as it lazily produces results.  So we can get laziness out of this.  There's a function called transduce, which is just like reduce, except it also takes a transducer so that takes a transducer, an operation, and initial value, and a source, so the transducer is a modification of processed bags, we'll talk about in a second.  The operation is some, the initial value is 0 and the source is the pallets, so what is this composition going to do?  It's going to sum the weight of the bags.  This is the weight of all the bags, right?  So it's cool, look, we can take the process that we already had and modify it a little bit.  We can add weigh the bags at the end of the set of instructions and that gives us a number and we can use that number with plus to build a sum.
>> So that's transduce, the other thing we can go is go to a completely different context now, so we have some channels we're going to be sending pallets of luggage across channels.  Of course we then really fit but the idea is there.  Channels run indefinitely but you can feed them all the time and get stuff out on the other end on a tenuous basis.  But these things are not parameterized.  They're not a thing that I can tell you later that you can tell me it's trolley or conveyor belt.  This is the exact same processed bags I defined here, this concrete thing, being reused in a completely different context so this is concrete reuse, not parameterization.  So we can use transducers on channels, the channel constructer now optionally takes the transducer and it will transduce everything that flows through.  It has its own be internal processing step and it's going to modify its inputs accordingly with the transducer it's given and it's an open system, I can imagine, but I did not get time to implement, that you could plug this in RX Java trivially and take half of the RX Java functions and throw them away, because you can just build a transducer and plug into them one is observable and returns an observable and that's the idea.  So we call all of these things into, sequence, transduce, Chan and transducer processes.  They accept a transducer so transducers sort of have two parts, you make functions that create transducers and in contexts where they make sense, you start accepting transducers, and then you have these two orthogonal Legos, you can put together.
>> And so what's the internal processing function of into?  The thing that adds one thing to a collection?  In Clojure called conj for conjoin.  Similarly inside lazily sequences there's some funk mechanism that produces on than and then waits to produce the next thing.  So channels also take inputs, somewhere inside channels is a little step function that adds an I put to a buffer.  That step function has exactly the same shape as conj and as laziness so it can transform its fundamental internal operation but the operation remains completely encapsulated.  the transducer in context takes the transducer, modifies its own step function and proceeds with that.
>> So as I said before, there's nothing new.  Two papers I find useful for helping you think about these things are this lectures in constructive functional programming, which is a lot closer to the source to when people started thinking about folds and their relationship to lists, and the second Graham Hutton paper is sort of a summary paper which summarizes the current thinking at the time it was written.  So they're both really good but now I'm going to take you through to how do we get to this point.  So one of the fundamental things that the bird paper and the one that preceded it talked about is these list operations in fold.  In fact there's a lot of interesting mathematics that show that they're the same thing.  Whether you can go backwards and forwards in terms of a concrete list and the operations that constructed it.  They're sort of isomorphic to each other, right?  So many of the list functions that we have, can be redefined in terms of fold, right?  There's already been a definition of map in several talks here, I think, but the traditional definition of map says, you know, if it's empty return empty.  Sequence if you're getting a new input, cons that input into the result of mapping to the rest of the input, right?  It's recursive and calls itself.  But you know, map does that, filter does that, map cat, they all sort of have those structures, but filter is a little bit different, it has a predicate inside, it has a conditional branch and then it recurses in two parts of the branch with different arguments.  So what this earlier work did is say you can think about all of these things as folds.  If you do, you get a lot of regularity and things you can prove about folds which are now all uniform will apply to all these functions which otherwise look different from each other so there's a lot of value to this.  If we look at a redefinition of map, it's not often defined this way, but if we look at a redefinition of map in terms of fold, then we say, we're going to fold this function that cons the first thing onto the rest and we start with an empty list, so this is fold, fold right, and we do this over a collection.  We can similarly define filter in this way.  That's all boilerplate, right, map and filter are precisely the same in those things.  All that's different is what's inside the inner function definition, and even there, there's something the same.
>> So it ends up that you can similarly redefine these functions, or define these functions in terms of fold out.  Fold out is just left reduce and so here are some what if definitions and we added map cat that are left folds that use left reduce, and so the tradeoff between left reduce and right reduce is right reduce sort of puts you on the laziness path and left reduce puts you on the loop path.  So we like that, so this means we can turn these things into loops.  Because reduce becomes a loop.
>> But the same thing, we have the boilerplate, we have reduce, right these definition the use vectors, which in Clojure they're like arrays, but their fundamental conjuring operation is at the end.  So this has the same shape I want to talk about for the rest of the talk.  We have something that we're building up, a new input and we produce a new thing and sort of the stuff is comes out of the right and getting added to the right side.  But it's the same idea.  We're reducing, we have a function that takes the vector so far and the new value.  We're conjoining the new value, having applied F to it, right, that's the idea of mapping, right?  There's an idea behind mapping that luggage handlers understand, right?  Put the label on everything that comes through.  It's very general, right?  That's mapping.  They get that, we get that.  As programmers we muck this up, because look at what's happening here.  Map says there's this fundamental thing that you do to everything as it comes through.  Filter says, there's this fundamental tiny thing that you do to everything as it comes through and map cat says there's this fundamental tiny thing that you do to everything as it comes through.  What's the problem?  Conj.  Conj is basically like saying to the trolley or to the conveyor belt.  It's something about the outer job that's leaked or it's inside the middle of the idea.  Inside the middle of the idea of mapping is this conj.  It doesn't belong.  Inside the middle of the filter is this conj, it shouldn't be there.  The same thing with map cat.  This is specific stuff in the middle of a general idea.  The general idea is it just take stuff out.  We don't want to know about conj, maybe we want to do something different, OK?  So again we have a lot of boilerplate, we have these essences and the other critical thing is the essences can be expressed as reducing function, each of these little functions is exactly the same shape as conj.  So to turn those inner functions into transducers, we're just going to parameterize that conj, right, we're going to parameterize the old fashioned way with the function argument.  Don't want anything higher order or blah blah blah, you know, we're going to take the argument, which is the step.  So right in the middle body of this mapping, -- this is the same as it was in the last slide, this is where it said conj, now we say step.  We put that inside a function that takes a step.  So this is a function, mapping takes the thing that you're going to map, you know, label the baggage, right, and it turns something that is a function that expects a step.  What are we doing?  Putting stuff on conveyor belts?  What are we doing?  We're putting stuff on trolleys, OK?  And it says before I do that, I'm going to call f on the luggage.  I'm going to put a label on the luggage.  I don't know about luggage anymore, the step you're going to tell me later, what are we doing today?  Conveyor belts or trolleys?  Conveyor belts, cool, I got the rules, I understand how to do mapping and filtering and map catting, so same thing filter.  What's beautiful about this is what's the essence of filtering?  Apply a predicate and maybe you do a step or maybe you don't.  There's no step here.  It's a choice about activity, it's a choice about action.  Same thing with concatenating, cat, what's it do?  Basically says do a step more than once.  I'm giving you input that's a set of things, do it to each thing and mapcatting is just composing map and cat, which it should be.
>> OK.  So we can take these transducer-returning functions so mapping returns a transducer, filtering returns transducer and cat is a transducer and map capping returns transducer.  Now that we've made mapping into this abstract thing that doesn't really know about lists or vectors anymore.  It says if you give me a step function, I'll modify it first to do f on the input.  OK here's the step function, conj, now I've rebuilt the functions that I've had before, except conj is not inside filtering and map catting anymore it's an argument.  Woohoo!! we now have the essence of these things aa la carte, and that's the point, right?  Transducers are fully decoupled they don't know what they're doing.  They don't know what process they're modifying.  The step function is completely encapsulated.  They have some freedom, they can call the step function not at all, once exactly per input or more than once per input but they don't really know what it does so that's what they're limited to doing, using it or not using it.  That's pretty much it except they do have access to the input, right, so when we said map cat unbundle pallet, the function we're supplying there is something that knows about pallets, it doesn't know about conveyor belts, it doesn't know what the overall job is but it knows about pallets and it's going to know how to turn a pallet into a set of pieces of luggage.  There's a critical thing about how they use that step function that they've been passed and it goes back to that step function I mentioned before.  They must pass the previous result from calling the step function as the next first argument to the next call to the step function.  That is the rule for step functions and their use and no others.  
     So let's talk a little bit about the backwards wards part because this is a frequent question I get, what did you do?  Do transducer change comp is the first thing.  They ruin comp or something like that.  And so what we have to do is look at what transducers do, right?  A transducer function takes a function, wraps it, and returns a new step function.  That is still happening right to left.  This is ordinary comp and it works right to left.  So mapping gets run first.  Right?  We're going to have some operation, put stuff on a trolley or conj.  Mapping will be the first thing that happens.  It's going to make a little modified step that labels the heavy bags before it calls put it on the airplane.  Then filtering gets called.  It does go right to left.  It says give me that step, I'll make you a new step that first sees if it's food.  If it's food, I'm going to throw it away.  If it's not food, I I'm going to use it.  Then map catting runs or the result of map catting runs and that says give me a step and I will take its input, presume it's a pallet, unbundle it and supply each of those arguments to the nested thing so the confirmation of the transformers run right to left but it.  
     In other words, comp is working ordinarily, it's building steps right to left, the resulting step runs the transformations left to right.  So when we actually run this, we'll unbundle this, call the step that says to unlabel the food, call the next step.  That's why it looks backwards.
>> The other nice things about transducers is there's no intermediate stuff.  They're short, they potentially could be inline.  There's no laziness required, there's no laziness utilized.  We're not going to tell it how to make everything into a list so you can say an empty list is nothing.  You know, nothing is nothing.  An empty list is an empty list.  And one thing is one thing.  A list of one thing is a list of one thing.  These are not the same.  So you use the step function or you don't and there's no extra boxes required or boxing for communicating about the mechanism.  
     So the other thing that was sort of interesting was you know, started talking about transducers and a lot of people in Haskell were trying to figure out what the actual types were because I had shorthand in my blog post and I'm not going to get into that right now, except to say that I think it's a very interesting type problem and I'm very excited to see how people do with it in their various languages.  I've seen results that are sort of it works pretty well to you know, these types are killing me.  
     Depending on whether the user's type system could deal with it, but let's just try to capture what we know so far graphically and somebody who reviewed these slides for me said these should have been subscript stuff but computers are so hard to use, I couldn't switch them in time so they're super scripts but the idea is that if you're trying to produce the next process N, you must supply the result from M minus 1 in the input.  If you're trying to model in your type system R to R, that's wrong, OK?  Because I can call the step function five times and then on the sixth time take the return value on the first time and pass it as the next time.  That's wrong.  So you got to make your type system say that's wrong.  So figure that out.  Also, if you make the black box and the black box the same thing that's arbitrarily restrictive.  That's it:  There's nothing wrong with that state machine.  It is a perfectly fine reducing function, it may, you know, be tough to model in a type system and don't say X or Y or Z.  When it's given it only returns Y.  It never returns Z.  So seems like a good project for the bar later on.  But the thing that we're capturing here is that the new step function might take a different kind of input.  It might take a B instead of an A.  Our first step does that.  It takes a pallet and it returns a set of pieces of luggage.  But each step returns a piece of luggage.  OK.
>> So, there are other interesting things that happen in processes, right?  Ordinary reduction processes everything, but we want this to be usable in cases that run arbitrarily long, we're not just talking about turning one kind of collection to another kind of collection, right?  A transducer that's running on a channel has got an arbitrary amount of stuff coming through.     
     On an event stream has an arbitrary amount of stuff coming through.  But sometimes in the producing process or somebody says whoa, I've had enough, I don't want to see any more input.  We're done.  I want to say we're done now even though you have more input so we're going to call that early termination.  And it may be desired by the process itself like the thing at the bottom, or it may be a function of one of the steps, one of the steps may say, you know what?  That's all it's supposed to do, and so I want want to see any more input.  We're going to modify our instructions if the bag is ticking, you're finished, go home, we're done loading the plane so we're going to add that.  Taking while nonticking.  And taking while nonticking needs to stop the job in the middle.  Doesn't matter if there's more stuff on the trolley, if it's ticking, we're finished, OK?  There's a constructer of a special wrapper object called reduced which says this represents the end of the -- this says I don't want to see any more input, here' what I've come up with so far and don't give me any more input.  And there's a predicate that allows you to ask if there's something in this wrapper and there's a way to unwrap this thing and look at what's in T.
>> This is not the same thing as maybe.  All right, because maybe also wraps the other things that are not reduced, right?  Or either or all of those other boxy kind of things.  So we don't do that, we just have a -- we only what we're doing this special termination.  So like reduce, transducers also must support reduce, that means that the step functions are allowed to return a reduced value and that if a transducing process or a transducer gets a reduced value, it must never call the step function with input again.  That's the rule.  Again, implement the rule in your type system, have at it, but that's the rule, OK, so now we can look at the insides of taking while.  It takes a predicate, it takes a step that we're going to modify, it runs the step on the input.  If it's OK, it runs the step.  If it's not OK it takes what has been built up so far and says we're finished, reduced result.  That's how we bail out.  But notice the ordinary result is not in a wrapper.  And so the reducing processes must also play this game, right?  The transducer has to play -- follow the rule from before and a reducing process similarly has to support reduce.  If it ever sees a reduce thing, it must never supply input again.  The D reference value is the final accumulated value but the final accumulated value is still subject to completion which I'm going to talk about in a second so there's a rule for the transducers as well.  So now we get a new pictorial type that is omnigraphal.  We can have a process, right that takes some black box at the prior step and an input and returns a black box at the next step or maybe, right, it returns a reduced version of that.
>> So one of those two things can happen, or vertical bars or.  Right, and it returns another step function that's similarly can take a different kind of input, a black box, returns a black box or reduced black box.  Same rules about successor ship apply, all right?
>> All right, so some interesting sequence functions require state.  And in the purely functional implementations they get to use the stack or laziness to put that state, right?  They get somewhere in the execution machinery a place to put stuff.  Now we're saying I don't want to be in the business of specifying if we're lazy or not lazy or recursive, right?  I'm not going to give you space inside the execution strategy because I'm trying to keep the execution strategy from you and that means that state has to be explicit when you have transducers, each transducer that needs state must create it.  So examples of functions that need state are take, partition, partition bind, things like that.  They're accounting or they're accumulating stuff to spit out later, where's that going to go?  And it has to go inside the transducer object, they have to have state.  You have if you need state you have to create it every time uniquely and again every time you're asked to transform a step function, so in new you're going to create a state every time you form a step section.  That means if you build up a transducer stack, some of which are staple transducers and you apply it, not when you build it, no state exists then.  When you've applied it, you now have a new process step but as we should be thinking about all transducer process steps, including the ones at the bottom, right, that may be stateful, you don't know at the very bottom process isn't you know, launch stuff into space, right?  
     So you should always treat a transducer stack as if -- what ends ups happening in practice is all of the transducer processes they do the applying.  It's not in the user's hands to do it.  You pass around a transducer and input into the job, the job applies the transducer to its process, gets a fresh set of state when it does that, and there's no harm.  But you do have to do this by convention.  So here's an example of a stateful transducer dropping while a predicate is true, right is this so we start with our flag that says its true.  It's as long as it's still true we drop.  
     When we see it's not true we're going to reset it and continue with applying the step and from then on forward we're going to apply the step.  So that is not the produce thing.  I talked before about completion so we had the idea of early termination.  The other idea transducer which is the completion.  There are plenty of jobs that don't have complete.  They're processing everything that comes through a channel or everything that comes through an event source, there's no end.  But for things that have an end there's a notion of completion which is to say, if either the innermost process step want to do something finally when everything is finished, they can, or if any of the transducers have some flushing they need to do, they can do it, right, so the process may want to do a final transformation on the output.  Any stateful transducer, in particular a transducer like partition, right, it's aggregating to return aggregates, you say a partition five and it collects five things and spits it out.  If you say we're done, we've got five things, it needs to spit out the five things.  But you need to be able to tell it we've exhausted input.  In order to do that the way it's implemented in the Clojure implementation of transducers is all the step functions enough a second operation.  So there's the operation that takes a new input in the accumulated value so far and returns a new accumulated value or whatever, I mean it's up to the process what the meaning of the black box is, but there must be another operation which takes just the accumulated value and no input.  So an arity1 operation.  So we'll talk about what that does or how it gets used.  
     If the process itself overall job is finished exhausted input it has a notion of being finished.  This is not bailing out.  This is like there's nothing more to do.  It must call the completion operation.  Exactly once on the accumulated value.  So there's no more inputs, I'm going to call you once with the completion.  It may, however, before it does that, flush.  So if you have something like partition, that's accumulated some stuff along the way, it can call the ordinary step function and then call complete hon the result.  And that's how we accomplish flushing.  There's just one caveat here which is if you're a stateful thing like partition and you've ever ever seen reduced come up, well, the earlier rule says you can never call the input function so just drop whatever you have hanging around, because somebody bailed out on this process, there's going to be no ordinary completion.  So we can look at our types again in omnigraphal 2000, latest programming innovation, and think about it as a reducing function as a pair of operation, right?  They'll be different in each programming language it's not really important.  In Clojure it ends up a single function can capture these arities.  It takes a pair of those things and returns a pair of those things and again we don't want to concretely parameterize that.  If you do that, you'll have something that only knows about transducing into airplanes, as opposed to the general instructions.
>> OK.  There's a third kind of operation that's associated with sort of processing in general.  Which is init.  We've had talks that mentioned monoids and things like that, the basic idea is sometimes it's nice for a transformation operation to carry around an initialization capability.  It may not be the identity value or anything like that.  It does not matter.  What does matter is a reducing function is allowed to, may, support arity 0, in other words, given nothing at all, here's an initial accumulated value from nothing.  Obviously a transducer can't do that because it's a black box and one thing it doesn't know is how to make a black box out of nothing.  Can't do it.  All it can ever do is call down to the nested function.  So transducers must support arity 0 in it, and just they can't really do it but they can carry it forward so that the resulting transducer also has an init if the bottom transducer has an init.
>> I'm over time already?  Sorry.  So here's an example, right, plus from Lisp.  This is older than transducers, Lisp programmers, plus with nothing returns the identity value for plus 0, multiplication with nothing returns 1, right it implements plus of an accumulated result as identity and the binary operation that does the work.  So here's the types again.  We now have an optional init from nothing and we're taking a set of three operations and returning a new set of three operations.  In Clojure we just use arity to do this.  We haven't actually called the reducing functions mapping and filtering and, ng this and ing that.  So we've modified so far all of these sequence functions to do that.  So this is the final example of filter returning a transducer.  Right?  
     It just takes a predicate and returns a step modifying function which takes a reducing function which presumably has these three arities, and Init, complete, filter doesn't have anything special to do so it just flows it through.  And we can see, we can define the collection implementing one by just calling sequence with this transducer and that's true of all of these functions, you can define the collection version exactly like this, which shows that transducer is more primitive than the other.  
     This is what you're trying to accomplish.  You define a set of transducers once, you define all of your cool stuff, channels today, observables tomorrow, whatever the next day, you just make it accept transducers and every specific implementation you get of these things you get it for free and every recipe that somebody creates works with your thing right away.  That's what we want, right?  
     We're going to take Perlis and say it's even better.  We want 100 data functions with no structure.  So transducers are context independent.  There's tremendous value in that.  They're concretely reusable.  So somebody can make this and not know how you're going to use it.  It's much stronger than parameterization, because you can flow it.  It supports early termination, they support early termination and completion, you can compose them just as easily as you can compose the other ones, they're efficient and tasty.  Thanks.
[break]

