
Strangeloop, 9/18/14

"Scaling League of Legends Chat to 70 Million Players"

Presenter: Michal Ptaszek

Live captioning by Norma Miller, whitecoatcaptioning.com

>> MICHAL PTASZEK:  Before we start, let me ask you a question.  How many of you have ever played a multiplayer theme game?  So as you probably are aware, in any team game, communication is extremely important.  If you could imagine a basketball game where no players to communicate with each other, it would be a disaster.  LoL of also a team-based game and communication is crucial for us, as well.  So today I would like to start with a short introduction to League of Legends.  I would like to tell a few words how you use chat for the game, then focus on the technology we use to view chat and then move forward to lessons learned, so what kind of difficulties we faced when building chat, how we solved them, and finalize with Q and A session.  So let's start with what is League of Legends?  So League of Legends is a team-based game from the MOBA, multiplayer online battle arena.  Typically in the game you would see two teams of five people competing with each other, trying to control a map, in the chief objectives, destroying an enemy base and also protecting their own base.  Every person playing the game is controlling a single champion, a single unit, and in LoL, we have over 100 champs right now.
>> And everything is based in the modern fantasy world.  So you would see words and shields and also magics, so where do we have chat in all of this?  So chat primarily acts as a messaging service for the game, so players are able to chat with each other using private one to one channels and also participate in group chat discussions so we this hook up with each other and start talking with each other.  The second very important thing that chat is doing is acting as a presence service.  That is, it controls your friend lists, it remembers who you friended in the past, and also displays their present status, that is, whether they are online or offline, and what is their state, are they playing the game, or are they AFK or maybe also things like what champion they are playing or for how long they've been playing.
>> And finally, chat is also used pretty extensively by other service it is at riot, so we expose a lot of RESTful APIs to other services on the back end, things like store, talks to chat to verify friendship between two players, or we have leagues that are trying to use our social graph in order to seed new players together so that they can play together more often and can compete with each other.
>> What we observed over time is also that whenever chat is not working or it's not working as expected, player experience is getting significantly worse, and what is also important to note is that chat not only has to be up and running, but it also has to provide low and stable for the players, so we cannot introduce lags in the communication, otherwise it wouldn't be effective.
>> So here is a few examples of how chat looks like in the game.  So on the very left -- yes, left side, you'd see your buddy list, so all the players you friended and their current status, and if you hover a mouse over one of the entries, you would see the detailed information, the presence, so the icon of the player, their level, their custom message they put, and also a few other things, and on the very right side you'll see a one to one chat window, so everything is using chat on the back end.
>> So to set the context for further discussions, here is the high level overview of the scale we need to operate at.  So we are very lucky to enjoy 67 million unique players playing League of Legends every month.  We can see 27 million players playing daily, and during the peak hours, we observe as many as 7.5 million players logged into the platform.  That translates roughly to the numbers we see on the chat side.  The number is slightly higher on the chat side because of the third party clients connecting to our services, and not using the game itself.  But that translates roughly to 1 billion events that each chat server, each box, has throughout every day.  If you average it to the seconds, a number of operations we have to process every second during the day, it's something around 11 thousand messages.  So our goal was to come up with a very stable and scalable chat service, chat solution, and over time we identified three components.  That would help us achieve it.
>>
>> First of all, we had to choose a protocol, a communication layer between clients and servers, and then we had to choose the server itself.  So the implementation, the routing engine, and finally, we had to choose data store.  Something that would persist all the information, we would like to keep in between player log-ins.  So let's start with the protocol.  For that we chose XMPP, extensible message and presence protocol.  It's basically a communication protocol based on XML and it's primarily used for instant messaging, for present information, for maintaining contacts list.  As it is stated in its name, its pretty extensible.  The community created a lot of extensions to the core of the protocol, which are called XMPP enhancement proposals.  At riot, we're trying to be as performant as possible and we have to diverge from the core of the protocol and we keep our own internal library of X apps.  On top of that, XMPP is also widely adapted on the market.  If you've ever used Facebook chat or Google talk or WhatsApp in the very early days, you used XMPP under the hood.
>> For example, even Skype has XMPP guideways, so in theory, you can connect from your avian or pigeon or whatever server you want to Skype servers.
>> The second component of our service is server itself.
>> For that, we chose ejabberd at the very beginning.  We started from there.  At the very beginning it offered us a very nice capability and performance with pretty much changes to the code or the configuration.  Nevertheless, we were extremely aware that one day, considering the growth and internal requirements, we would have to get our hands dirty and start modifying it.
>> So as a fun fact, ejabberd is written in exotic language called Erlang, you've probably heard of, so a couple of words on it.  So at first Erlang is a functional language.  It's it gives us a much faster way to build prototypes and it also imposes a declarative style of programming.  It's also built with distribution, concurrency and scalability in mind.  The language itself has language level constructs allowing you to spawn processes, control processes, control tolerance, both locally and globally if you run DOMs in a cluster, which in turn gives us more time to focus on the actual application code as opposed to fixing or coming up with solutions for scalability problems.
>> Finally, which is super appealing to me, is an ability to perform hot code reloading, so whenever you find a bug in your code that is deployed in production, you don't have to stop a service, you just need to upload the patched file, the patched module to all your services.
>> And we load it on the fly.  That way, we can eliminate down time entirely.
>> So when it comes to the architecture of ejabberd or the original ejabberd and the ejabberd we use now, we were looking at enabling systems that share nothing in between and that applies to both internal and external facing interfacing and servers.  It allows us for better isolation and trace ability, and it also goes well with the scale we need to operate at.  It's our aspirational state, we are not really there yet, but we are slowly but surely progressing in that direction.  More on that in the next slides.
>> Next, because we have deployed a few hundred chat servers all over the world and our team consists only of three people doing that all, we need to make sure that our service is able to self-heal.  We do not have to react whenever we have a minor failure.  It will auto repair under the hood.  So if you think about user sessions, so every time user connects with a server, we have his session living on the back end.  Whenever that user messes up with something, starts to break a service or whatever, that actions will only affect his session context.  The error will not propagate to any other users or any other services running on that server.  And whenever that user session crashes for some reason, the error will not explode and will not take his whole service down.  Instead, that error will be isolated to the session only and will be garbage-collected, that session will be garbage-collected later by our internal processes.
>> The next thing that is quite opposite to the fault tolerance is the dreaded crash philosophy which says that whenever we have a major failure, we do not try to slowly recover from it.  Instead, we let the system or the part of the system crash, and we can easily restart it to the known stable state.  That way, for instance, whenever we have a huge lag of pending queries on our database drivers, instead of waiting for them to drain over time, over a few hours, what we typically do is we restart the database driver and all the new queries will be processed in the real time, while the queries that were queued up in the past will be rescheduled for processing later, so the way we built the application, it is possible to reschedule queries for later processing.
>> So here's how our services work or how they look like.  So on the very left side, you can see two physical servers, and on each of these servers, we run through OS processes, one is ejabberd, and the other so that part, the server part is very easily scalable, we can add as many servers as we want and we increase capacity as we add them, so they run in a cross server mode, ejabberd is able to talk to other ejabberd servers.  That allows us to run costly EPL queries on our social graph, as well as perform backups, without interrupting the live service.
>> When it comes to implementation, as I mentioned earlier, we had to focus a lot on performance and scalability and fault tolerance and over time, little by little, we pretty much rewrote the entire ejabberd code.  Today if you asked me to estimate how much is left from the original open source implementation, I would probably say 5 to 10%.  So 90 to 95% of the code is ours and it's custom to Riot requirements.  So over time we removed unwanted unnecessary code.  We optimize the protocol itself and the existing code a lot and we wrote a lot of tests to make sure we didn't break anything as we go forward.  To give you an example of protocol optimizations, XMPP allows you to have asymmetrical friendships between users, so if Alice is a friend of Bob, Bob doesn't have to be a friend of Alice.  For League of Legends, we only allow players to be mutual friends.  So if Alice is a friend of Bob, Bob is automatically a friend of Alice, so XMPP, in order to establish a fully functional friendship, required us to send something about like 16 messages back and forth between clients and servers.  And that was hitting our data stores pretty bad.  What we've done instead, we've changed the protocol a bit, and we made it that way so whenever Alice sends an invite to Bob, and Bob replies, the friendship is already established.  We don't have to shake hands once more.  The next thing we had to do was to profile the entire code and look for obvious bottlenecks.  Things that do not scale, that share state, across multiple processes or multiple nodes, and tried to make it so that it can scale linearly on single server, as well as in the clustered environment.
>> So as an example again, here, a MUC is an abbreviation for multiuser chat.  So every chat server can handle hundreds of thousands of user connections and for every user connection we have a session process.  Every time a user wanted to send a message or post his presence update to one of the rooms he was a member of that, event had to go through a single process called MUC router, that MUC router was basically a messenger that was relaying all the messages to particular group chats, on a multi-core system that was a disaster, if you think about a critical section that's over a few hundred lines of code in your C++ application.  So it was a clear bottle neck to us, and what we've done is reparallelized the routing and the whole routing process happens in the context of using session, so that now we are able to.
>> The other thing we implemented for chat was every ejabberd server contains a copy of so-called session table.  So session table, you can think of it as a translation table between jabber identifiers, and session handlers, session processes.
>> So whenever you want to send a message to someone, you know his JID, jabber identifier, and you look up his session in the cluster.  So whenever a user was posting a presence update, say Bob finished his game, and he went back to the game lobby, so he posted an update.  ejabberd was writing down that presence update to the session table.  So effectively, even though his session handler didn't change, there was a right to that table.  By first checking if the exists, and checking that the presence priority is correct and stuff like that, were able to reduce the number of distributed rights to that table by I think 96%.  Which was a huge win for us.  We were able to log in more users, faster and we were able to also allow users to change their presence updates much more frequently.
>> And finally, we had to take a closer look at Erlang PM itself and the OTP libraries it comes with.  And so we were aware that by changing the PM, the server libraries, we are sacrificing the generic nature of our own IPE in favor of better performance or better scalability.  So on the application level, the libraries level, we tried to get more visibility into what's going on whenever we deployed a new code to production, whenever we hot code relead.  In the past, the original Erlang OTP framework, whenever you upload, release upgrade package to t it would only output two messages.  First one would be OK I'm beginning to migrate your system from version A to version B, and the second message would be it was successful or it failed.  Whenever we needed to change, say, 100 files, getting a message saying it failed, it doesn't really help us, so we added a lot of logging, a lot of visibility lines into that process so that now we can keep track of progress on every single file level and we can also have an ability to reload the code on multiple servers at once in a transactional context.  The other thing we had to do for optimizing our servers was to build in the back functionality into the VM itself, so to keep the memory footprint low for every session handler we had to have an ability to inspect what the process was allocating and what it was wasting energy on.  So for the early VM, we added a functionality to inspect the heaps of the processes so that you can point to a session handler process and ask what kind of terms that process allocated has on his heap and how many machine words each term consumes.  That way, we have a clear picture of memory utilization, and also what are the parts of the session state we should take a closer look at.
>> Finally, data store for chat.  With the number of players Riot enjoys, we knew that scalability would be a problem from the start.  We opted in for no mySQL.  Initially, we started with mySQL, however, over time, we are stumbling on more more people performance issues, stability issues, and flexibility issues, when we were not able to manipulate mySQL schema fast enough to keep up with the changes we made in the code.  So we chose React, React is a distributed fault tolerance key value storage.  It's truly masterless, so whenever we have an issue in live service, when one of our server die, there is no single point of failure.  We can afford losing two of the servers on production, and still be able to operate without losing any data, without losing any functionality.
>> We have implemented toll.  So for that, we had to spend a lot of time on the client side on the client from the React perspective, on the server side, the chat server, and we implemented ejabberd level CRBP library.  Communicative replicated data times.  So that library takes care of all the right conflicts that happen in React, it tries to converge the objects to the stable predictable state.
>> Over time, it proved to be a huge success, it allowed us to scale linearly, and it also gave us a flexibility in terms of changing the object layouts and playing around with features.
>> Nevertheless, it required a huge paradigm shift.  It required a lot of work on our end to change our way of thinking, and also how we test our services, and how we build tools around it.
>> Let's move to lessons learned.  So first of all, we realized the key to success is to understand what our system is actually doing.  So monitoring is key to see whether your system is in a healthy state, whether it's about to crash, or whether it's being attacked by some malicious users.  So we've built over 500 real-time counters, rates, histograms into ejabberd, they are all collected every minute and posted to either ZABBIX or graphite that we use for monitoring.  For every counter and every that we collect, we have normal or regular conditions as well as abnormal conditions, so any whenever any of the counter values goes way off the limits, it is NOC is automatically notified by that fact and we can react pretty fast, long before the players realize there is a problem will be on the surface.  So a fun story about it is a few months back in time, we released a very buggy client update.  The client code itself was pretty much broadcasting its own presence every time it was receiving a presence update from someone else.  So you can imagine an infinity loop of presence updates flying around the network, so we were wondering why our chat servers were so hammered by the load and briefly looking at the graph at graphite, we were immediately able to realize, OK, that's additional traffic is actually presence or presence updates, and they started to show up exactly  at the time when we released the new client update and after patching the client, which was relatively easy, we were almost immediately able to verify that the fix worked and the presence -- the rate of presence updates went back to the normal level.
>> Another thing we found very useful when building new features is to have an ability to turn them on and on off on the fly without having to restart the service itself.  So whenever we come up with something new, we're typically surrounded with toggles, and that way, if that feature causes any problems, we can safely disable it.
>> Yet another thing we built for our chat servers is an ability to perform partial deployment, that is, we can get new code and enable it only for selective users.  For example, we can provide a list of user accounts that would have that particular feature enabled, or we can express that only 1% of our player population will see that feature in live.  That allows us to test potentially dangerous features at the lower scale, and if they behave OK, we can then turn it on for all the players.
>> Another thing, which I mentioned earlier, is ability to code reload on the fly.  So a few times in the past, we were unfortunate to release code that wasn't really well tested on the third party clients.  So it was working fine with the initial client, however, ADU or PSI or pigeon were sending different type of chats to the client servers than our client and that was causing problems.  So we had either an option to patch the buggy code, perform the server restart, and run the fixed code, or what we could do with Erlang and with the way we rebuilt our release upgrade system was we were able to upload our changed modules to the servers and apply them without having to restart the whole chat.  That of course translates to lower downtime for the players.
>> The next thing is logging.  It's pretty obvious, but it's really important, as well to get it right.  So for that we not only tried to log all the abnormal conditions, like errors and warnings, but also tried to make sure we keep track of healthy state in the server.  So servers report healthy state over time, and that way by briefly looking at the logs, we can immediately identify, OK, that server is OK, because it's logging users, it's accepting new connections, it's modifying friends lists, for example.  On top of that, we built a functionality to enable back modes for receptive server monitors.  So for example, if you have an experimental user who is testing his things on production servers, we can turn on the back mode for that particular session, and that way even though we have 100,000 players connected to that particular ser, only that session is emitting megabytes of logs, including all the traffic and all the metrics related to that particular user.  That way, if you combine that with the feature toggles and deployments, we can actually deploy the  server and let it be tested only by a few people that will emit tons of logs to us so that we can take it back to our development machines and optimize it or change if it is buggy.
>> Then always load test your code.  So by that we mean that we have an automatic verification system of our latest changes.  Every time we change something, that very night we have that picks up the code, deploys it and runs a battery of low tests against it.  It's not only spinning up clients that are hitting the chat servers, it's also monitoring the health of the server.  So it's pulling the metrics from it, it's analyzing them, it's creating that beautiful confluence page with tables and all the counters, metrics and everything, and also in the morning, we find an email summary in the email box, saying that this was successful or it failed.  We also have an ability to compare our changes to the previous builds so that we can keep track of the impact of our code changes, whether it was a disaster or whether we improved memory consumption by 50%.
>> And lastly, things will always fail.  I mean, we have no control over everything around us.  You can think about it that in a way that even though in theory we can come up with a bug list code on the server side, there will be external factors, such as ISPs.  What happens if one of the main routers at the ISP site dies and we lose 100,000 players at the same time?  Or if the hardware melts or if we have problems in the network?
>> So you have to be prepared for that situation, and you have to make sure that your service is able to handle losing half of your logged-in players at once, or is able to handle losing 5 out of 20 chat servers without degrading performance.  Also, no code is bugless, so even if we have a code that works in most of the cases, say a bug happens once every 1 billion times at our scale, it will always happen on each chat servers once a day, so even unlikely events will always happen over time.  So where are we now?  Chat is doing great.  We can enjoy over 99% of quality up time.  Our aspirational state is to reach the mythical 99 of availability.  We are slowly going there.  When it comes to scale of performance, we have several vers in.  As I say, each chat server dials with 1 billion events a day and that happens with relatively low resource utilization.  It's something between 20 and 30% of available CPU and RAM.  We are migrating data off SQL worldwide, we are still in the transitional state.  The second thing we would like to do is make League of Legends chat available outside of the client, outside of the game, so that players with enjoy the friendships without having to log into the game itself.  And finally, on the back-end side, we are exploring possibilities around reusing our social graphs to make the whole experience better.  To analyze the connections between players and understand how they impact their enjoyment in the game itself.
>> That's it.  Thank you for listening.  And I would love to hear some questions.
>>
[applause].
AUDIENCE MEMBER:  Can you describe your -- [inaudible]
   So the question was can I describe nor CRTs in more details.  So the concept we use is instead of storing object values, say you have Alice and her friends lists, like it has Bob, Charlie and Eve, we don't store and every time we update that list by appending new item to that list, adding new friend, we don't store the list itself, the new value.  Instead.  We maintain an operational log to that objectable object, which will say, add Bob, so whenever two updates happen at the same time to a particular object, we would he end up with two entries appended to the object log.  You would have add player 1 and add player 2.  So the next time we read the contact we reflect a conflict to that object, we would take the two objects and merge the operational logs, and apply them in any order, because the order doesn't matter.  That way, we end up with a consistent state of friend list.  So basically the concept is you do not update the value, the list stored in data store in place.  Instead, you build a long log of operations to that object and you apply that operations whenever you read the object itself.
>> Right.
AUDIENCE MEMBER:  [inaudible]
>> So currently -- OK, so the question was, whether the in-game messaging is similar to out of game chat.  So currently the in-game chat is using game servers to route the messages between players.  However, I think the plans are to migrate it off the game servers to the chat servers themselves, so that we can use all the goodies we have today for out of game chat for the in-game chat, as well.
>>
AUDIENCE MEMBER:  [inaudible]
>> Oh, so that's totally different thing.  So that's totally out of scope of chat and that is dealt -- that's dealing with the game server itself.  Anyone else?
>>
>> All right, thank you all ...
[break]

